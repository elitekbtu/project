#!/usr/bin/env python3
"""
Enhanced Lamoda Parser Agent

–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ —Å Lamoda —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
- –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–∞—Ä—Å–∏–Ω–≥–∞
- –û–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- –î–µ—Ç–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
"""

import asyncio
import json
import re
import random
import time
import logging
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Union
from urllib.parse import urljoin, urlparse
from datetime import datetime, timedelta

import httpx
from bs4 import BeautifulSoup
import aiofiles
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–æ–º–µ–Ω–æ–≤
LAMODA_DOMAINS = {
    "ru": {"host": "https://www.lamoda.ru", "currency": "‚ÇΩ"},
    "kz": {"host": "https://www.lamoda.kz", "currency": "‚Ç∏"},
    "by": {"host": "https://www.lamoda.by", "currency": "—Ä."}
}

KNOWN_BRANDS = [
    'Nike', 'Adidas', 'Puma', 'Reebok', 'Jordan', 'Converse', 'New Balance', 
    'Vans', 'Under Armour', 'Asics', 'Mizuno', 'Skechers', 'Fila', 'Kappa', 
    'Umbro', 'Diadora', 'Calvin Klein', 'Tommy Hilfiger', 'Lacoste', 'Hugo Boss',
    'Demix', 'Outventure', 'Baon', 'Befree', 'Mango', 'Zara', 'H&M', 'Uniqlo',
    'Euphoria', 'Profit', 'Terranova', 'Pepe Jeans', 'Marco Tozzi', 'Tamaris',
    'Founds', 'Nume', 'Shoiberg', 'T.Taccardi', 'Abricot', 'Pierre Cardin',
    'Guess', 'Levi\'s', 'Jack & Jones', 'Only', 'Vero Moda'
]

@dataclass
class ParsedProduct:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞–∑–æ–≤ (5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π)"""
    sku: str
    name: str
    brand: str
    price: float
    old_price: Optional[float] = None
    url: str = ""
    image_url: str = ""
    image_urls: List[str] = field(default_factory=list)
    description: Optional[str] = None
    category: Optional[str] = None  # –û–¥–Ω–∞ –∏–∑ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π: top, bottom, footwear, accessory, fragrance
    clothing_type: Optional[str] = None  # –¢–æ –∂–µ —á—Ç–æ –∏ category –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    color: Optional[str] = None
    sizes: List[str] = field(default_factory=list)
    style: Optional[str] = None
    # –£–±—Ä–∞–Ω–æ –ø–æ–ª–µ collection - –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —É–¥–∞–ª–µ–Ω—ã –∏–∑ —Å–∏—Å—Ç–µ–º—ã
    rating: Optional[float] = None
    reviews_count: Optional[int] = None
    parse_quality: float = 0.0  # –ö–∞—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç 0 –¥–æ 1
    parse_metadata: Dict[str, Any] = field(default_factory=dict)
    parsed_at: datetime = field(default_factory=datetime.now)

@dataclass
class ParsingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    products: List[ParsedProduct]
    total_found: int
    success_count: int
    failed_count: int
    quality_score: float
    parsing_time: float
    metadata: Dict[str, Any] = field(default_factory=dict)

class EnhancedLamodaParser:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Lamoda —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
    - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
    - –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—à–∏–±–æ–∫
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    - –î–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    """

    def __init__(self, domain: str = "kz", cache_enabled: bool = True):
        if domain not in LAMODA_DOMAINS:
            raise ValueError(f"Unsupported domain: {domain}")
        
        self.domain = domain
        self.base_url = LAMODA_DOMAINS[domain]["host"]
        self.currency = LAMODA_DOMAINS[domain]["currency"]
        self.cache_enabled = cache_enabled
        self.cache = {}
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8,kk;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Chromium";v="122", "Not(A:Brand";v="24", "Google Chrome";v="122"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
        }
        
        self.session = None
        self.stats = {
            'requests_made': 0,
            'cache_hits': 0,
            'parse_errors': 0,
            'recovery_attempts': 0
        }

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.images_dir = Path("uploads/items")
        self.images_dir.mkdir(parents=True, exist_ok=True)

    async def _get_session(self) -> httpx.AsyncClient:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å HTTP —Å–µ—Å—Å–∏—é"""
        if self.session is None:
            timeout = httpx.Timeout(30.0, connect=15.0)
            self.session = httpx.AsyncClient(
                headers=self.headers,
                timeout=timeout,
                follow_redirects=True,
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            )
        return self.session

    async def _make_request(self, url: str, **kwargs) -> Optional[httpx.Response]:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å HTTP –∑–∞–ø—Ä–æ—Å"""
        session = await self._get_session()
        
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É
            await asyncio.sleep(random.uniform(0.5, 1.5))
            
            response = await session.get(url, **kwargs)
            self.stats['requests_made'] += 1
            
            if response.status_code == 429:  # Rate limiting
                logger.warning("Rate limited, waiting...")
                await asyncio.sleep(random.uniform(5, 10))
                return await self._make_request(url, **kwargs)
            
            if response.status_code in [200, 301, 302]:
                return response
            else:
                logger.warning(f"HTTP {response.status_code} for {url}")
                return None
                
        except Exception as e:
            logger.error(f"Request failed for {url}: {e}")
            return None

    async def parse_catalog(self, query: str, limit: int = 20, page: int = 1) -> List[ParsedProduct]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–∞–ª–æ–≥–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        start_time = time.time()
        
        try:
            logger.info(f"üîç Starting catalog parsing for query: '{query}' (limit: {limit}, page: {page})")
            
            # –°–æ–∑–¥–∞–µ–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞
            search_url = f"{self.base_url}/catalogsearch/result/"
            params = {
                'q': query,
                'submit': 'y'
            }
            
            if page > 1:
                params['p'] = page
            
            logger.info(f"Making request to: {search_url}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            response = await self._make_request(search_url, params=params)
            if not response:
                logger.warning("Failed to get response from Lamoda")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π
            products = []
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: JSON –∏–∑ —Å–∫—Ä–∏–ø—Ç–æ–≤
            logger.info("üîç Trying JSON extraction...")
            json_products = self._extract_json_products(soup)
            
            for i, json_item in enumerate(json_products[:limit]):
                product_data = self._parse_product_from_json(json_item, i)
                if product_data:
                    parsed_product = self._convert_to_parsed_product(product_data, 'json')
                    if parsed_product:
                        products.append(parsed_product)
                        logger.info(f"‚úÖ Parsed from JSON: {parsed_product.brand} - {parsed_product.name} - {parsed_product.price}‚Ç∏")
            
            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: HTML –ø–∞—Ä—Å–∏–Ω–≥ (–µ—Å–ª–∏ JSON –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
            if not products:
                logger.info("üîç Trying HTML parsing...")
                product_elements = self._find_product_elements(soup)
                
                for i, element in enumerate(product_elements[:limit]):
                    product_data = self._parse_product_from_element(element, i)
                    if product_data:
                        parsed_product = self._convert_to_parsed_product(product_data, 'html')
                        if parsed_product:
                            products.append(parsed_product)
                            logger.info(f"‚úÖ Parsed from HTML: {parsed_product.brand} - {parsed_product.name} - {parsed_product.price}‚Ç∏")
            
            logger.info(f"üìà Successfully parsed {len(products)} products")
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
            enhanced_products = []
            for product in products:
                if product.image_url or product.image_urls:
                    # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    product_dict = {
                        'sku': product.sku,
                        'image_url': product.image_url,
                        'image_urls': product.image_urls
                    }
                    
                    downloaded_data = await self._download_product_images(product_dict)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–¥—É–∫—Ç —Å –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏
                    product.image_url = downloaded_data.get('image_url', product.image_url)
                    product.image_urls = downloaded_data.get('image_urls', product.image_urls)
                
                enhanced_products.append(product)
            
            return enhanced_products[:limit]
            
        except Exception as e:
            logger.error(f"Catalog parsing error: {e}")
            return []

    def _convert_to_parsed_product(self, product_data: Dict[str, Any], method: str) -> Optional[ParsedProduct]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–∞ –≤ ParsedProduct –æ–±—ä–µ–∫—Ç —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            if not all([
                product_data.get('name'),
                product_data.get('price'),
                product_data.get('sku')
            ]):
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            raw_category = product_data.get('category')
            raw_clothing_type = self._extract_clothing_type(product_data['name'])
            normalized_category = self._normalize_category_for_outfits(
                raw_category, 
                raw_clothing_type, 
                product_data['name']
            )
            
            # –°–æ–∑–¥–∞–µ–º ParsedProduct —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
            return ParsedProduct(
                sku=product_data['sku'],
                name=product_data['name'],
                brand=product_data.get('brand', 'Unknown'),
                price=float(product_data['price']) if product_data['price'] else 0.0,
                old_price=float(product_data['old_price']) if product_data.get('old_price') else None,
                url=product_data.get('url', ''),
                image_url=product_data.get('image_url', ''),
                image_urls=product_data.get('image_urls', []),
                category=normalized_category,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞–∑–æ–≤
                clothing_type=normalized_category,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                parse_quality=0.9 if method == 'json' else 0.7,
                parse_metadata={
                    'method': method, 
                    'images_count': len(product_data.get('image_urls', [])),
                    'raw_category': raw_category,
                    'normalized_category': normalized_category
                }
            )
            
        except Exception as e:
            logger.error(f"Error converting product data: {e}")
            return None

    async def _parse_from_json_scripts(self, soup: BeautifulSoup, limit: int) -> List[ParsedProduct]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ JSON —Å–∫—Ä–∏–ø—Ç–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        products = []
        
        try:
            scripts = soup.find_all('script', type='application/ld+json')
            scripts.extend(soup.find_all('script', string=re.compile(r'window\.__INITIAL_STATE__')))
            scripts.extend(soup.find_all('script', string=re.compile(r'"products"')))
            
            for script in scripts:
                if not script.string:
                    continue
                
                try:
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON —Å —Ç–æ–≤–∞—Ä–∞–º–∏
                    content = script.string.strip()
                    
                    # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                    json_patterns = [
                        r'"products"\s*:\s*(\[.*?\])',
                        r'window\.__INITIAL_STATE__\s*=\s*({.*?});',
                        r'window\.__NUXT__\s*=\s*({.*?});'
                    ]
                    
                    for pattern in json_patterns:
                        matches = re.findall(pattern, content, re.DOTALL)
                        for match in matches:
                            try:
                                if match.startswith('['):
                                    data = json.loads(match)
                                    if isinstance(data, list):
                                        products.extend(self._parse_products_from_json_array(data, limit - len(products)))
                                else:
                                    data = json.loads(match)
                                    found_products = self._extract_products_from_json_object(data, limit - len(products))
                                    products.extend(found_products)
                                
                                if len(products) >= limit:
                                    return products[:limit]
                                    
                            except json.JSONDecodeError:
                                continue
                                
                except Exception as e:
                    logger.debug(f"Script parsing error: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"JSON script parsing failed: {e}")
        
        return products

    async def _parse_from_html_structure(self, soup: BeautifulSoup, limit: int) -> List[ParsedProduct]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        products = []
        
        try:
            # –ò—â–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤
            selectors = [
                'a[href*="/p/"]',  # –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
                'article[data-testid*="product"]',
                'div[class*="product-card"]',
                'div[class*="catalog-item"]',
                'div[data-testid*="product"]',
                '.x-product-card',
                '.product-tile'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if len(elements) > 3:  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    logger.info(f"Using selector: {selector} ({len(elements)} elements)")
                    
                    for i, element in enumerate(elements[:limit]):
                        try:
                            product = self._parse_product_from_element(element, i)
                            if product and product.price > 0:
                                products.append(product)
                                
                        except Exception as e:
                            logger.debug(f"Element parsing error: {e}")
                            continue
                    
                    if products:
                        break
                        
        except Exception as e:
            logger.error(f"HTML structure parsing failed: {e}")
        
        return products

    async def _parse_from_product_cards(self, soup: BeautifulSoup, limit: int) -> List[ParsedProduct]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏–∑ –∫–∞—Ä—Ç–æ—á–µ–∫ —Ç–æ–≤–∞—Ä–æ–≤"""
        products = []
        
        try:
            # –ò—â–µ–º –∫–∞—Ä—Ç–æ—á–∫–∏ —Å –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏
            card_selectors = [
                '.product-card',
                '[class*="ProductCard"]',
                '[data-testid*="product-card"]',
                '.catalog-item',
                '.item-card'
            ]
            
            for selector in card_selectors:
                cards = soup.select(selector)
                if cards:
                    logger.info(f"Found {len(cards)} cards with selector: {selector}")
                    
                    for i, card in enumerate(cards[:limit]):
                        try:
                            product = self._parse_product_card(card, i)
                            if product:
                                products.append(product)
                        except Exception as e:
                            logger.debug(f"Card parsing error: {e}")
                            continue
                    
                    if products:
                        break
                        
        except Exception as e:
            logger.error(f"Product card parsing failed: {e}")
        
        return products

    async def _parse_from_text_patterns(self, soup: BeautifulSoup, limit: int) -> List[ParsedProduct]:
        """Fallback –ø–∞—Ä—Å–∏–Ω–≥ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        products = []
        
        try:
            page_text = soup.get_text()
            currency_symbol = self.currency
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
            patterns = [
                rf'(\d{{1,3}}(?:\s+\d{{3}})*)\s*{re.escape(currency_symbol)}\s+([A-Z][A-Za-z\s&\.]+?)\s+([\w\s\-–∞-—è—ë\.,"\'()]+?)(?=\d{{1,3}}(?:\s+\d{{3}})*\s*{re.escape(currency_symbol)}|$)',
                rf'([A-Z][A-Za-z\s&\.]+?)\s+([\w\s\-–∞-—è—ë\.,"\'()]+?)\s+(\d{{1,3}}(?:\s+\d{{3}})*)\s*{re.escape(currency_symbol)}'
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, page_text, re.MULTILINE | re.IGNORECASE)
                
                for i, match in enumerate(matches[:limit]):
                    try:
                        if len(match) == 3:
                            if pattern.startswith(r'(\d'):  # –¶–µ–Ω–∞ –ø–µ—Ä–≤–∞—è
                                price_str, brand, name = match
                            else:  # –¶–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è
                                brand, name, price_str = match
                            
                            price = float(price_str.replace(' ', ''))
                            if 100 <= price <= 10000000:
                                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å–∏—Å—Ç–µ–º–æ–π –æ–±—Ä–∞–∑–æ–≤
                                normalized_category = self._normalize_category_for_outfits(
                                    None, None, name.strip()
                                )
                                
                                product = ParsedProduct(
                                    sku=f"TXT{self.domain.upper()}{i+1:04d}",
                                    name=name.strip()[:100],
                                    brand=self._normalize_brand(brand.strip()),
                                    price=price,
                                    category=normalized_category,
                                    clothing_type=normalized_category,
                                    parse_quality=0.3,  # –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
                                    parse_metadata={'method': 'text_patterns', 'normalized_category': normalized_category}
                                )
                                products.append(product)
                                
                    except Exception as e:
                        logger.debug(f"Text pattern parsing error: {e}")
                        continue
                
                if products:
                    break
                    
        except Exception as e:
            logger.error(f"Text pattern parsing failed: {e}")
        
        return products

    def _parse_products_from_json_array(self, data: list, limit: int) -> List[ParsedProduct]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ JSON –º–∞—Å—Å–∏–≤–∞"""
        products = []
        
        for item in data[:limit]:
            try:
                if isinstance(item, dict):
                    product = self._parse_product_from_json(item)
                    if product:
                        products.append(product)
            except Exception as e:
                logger.debug(f"JSON item parsing error: {e}")
                continue
        
        return products

    def _extract_products_from_json_object(self, data: dict, limit: int) -> List[ParsedProduct]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤ JSON –æ–±—ä–µ–∫—Ç–µ"""
        products = []
        
        def search_recursive(obj, path=""):
            nonlocal products
            
            if len(products) >= limit:
                return
                
            if isinstance(obj, dict):
                # –ò—â–µ–º –∫–ª—é—á–∏ —Å —Ç–æ–≤–∞—Ä–∞–º–∏
                for key in ['products', 'items', 'catalog', 'results', 'data']:
                    if key in obj and isinstance(obj[key], list):
                        found = self._parse_products_from_json_array(obj[key], limit - len(products))
                        products.extend(found)
                        if len(products) >= limit:
                            return
                
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º –≤ –¥—Ä—É–≥–∏—Ö –∫–ª—é—á–∞—Ö
                for key, value in obj.items():
                    if key not in ['products', 'items', 'catalog', 'results', 'data'] and len(products) < limit:
                        search_recursive(value, f"{path}.{key}")
                        
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    if len(products) >= limit:
                        break
                    search_recursive(item, f"{path}[{i}]")
        
        search_recursive(data)
        return products

    def _parse_product_from_json(self, item: Dict[str, Any], index: int = 0) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–∞ –∏–∑ JSON —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            product_data = {
                'name': None,
                'brand': None,
                'price': None,
                'old_price': None,
                'url': None,
                'image_url': None,
                'image_urls': [],
                'sku': None,
                'description': None,
                'category': None,
                'clothing_type': None,
                'color': None,
                'sizes': [],
                'style': None,
                'rating': None,
                'reviews_count': None
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            product_data['sku'] = item.get('sku', f"JSON{index+1:04d}")
            product_data['name'] = item.get('name', '')
            
            # –ë—Ä–µ–Ω–¥ –∏–∑ –æ–±—ä–µ–∫—Ç–∞ brand
            if 'brand' in item and isinstance(item['brand'], dict):
                product_data['brand'] = item['brand'].get('name', 'Unknown')
            else:
                product_data['brand'] = item.get('brand', 'Unknown')
            
            # –¶–µ–Ω–∞
            price = item.get('price_amount', item.get('price', 0))
            if isinstance(price, str):
                try:
                    price = float(price)
                except ValueError:
                    price = 0
            product_data['price'] = price
            
            # –°—Ç–∞—Ä–∞—è —Ü–µ–Ω–∞
            old_price = item.get('old_price_amount', item.get('old_price'))
            if old_price and isinstance(old_price, str):
                try:
                    old_price = float(old_price)
                except ValueError:
                    old_price = None
            product_data['old_price'] = old_price
            
            # URL —Ç–æ–≤–∞—Ä–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ–ª—è
            url = ""
            if 'url' in item and item['url']:
                candidate_url = item['url']
                if candidate_url.startswith('/'):
                    url = f"{self.base_url}{candidate_url}"
                elif candidate_url.startswith('http'):
                    url = candidate_url
            
            # –ï—Å–ª–∏ –Ω–µ—Ç, —Å—Ç—Ä–æ–∏–º URL –∏–∑ SKU + seo_tail (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç Lamoda)
            if not url and product_data['sku']:
                seo_tail = item.get('seo_tail', '')
                if seo_tail:
                    url = f"{self.base_url}/p/{product_data['sku'].lower()}/{seo_tail}/"
                else:
                    url = f"{self.base_url}/p/{product_data['sku'].lower()}/"
            
            product_data['url'] = url
            
            # –£–õ–£–ß–®–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º –ø–∞—Ä—Å–µ—Ä–µ)
            found_images = []
            
            def normalize_image_url(img_url):
                """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç img600x866"""
                if not img_url or not isinstance(img_url, str):
                    return None
                
                img_url = img_url.strip()
                if not img_url:
                    return None
                
                # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
                if img_url.startswith('//'):
                    full_url = 'https:' + img_url
                elif img_url.startswith('/'):
                    full_url = f"https://a.lmcdn.ru{img_url}"
                else:
                    full_url = img_url
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ Lamoda
                if (('lmcdn.ru' in full_url or 'lamoda' in full_url) and
                    any(ext in full_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp'])):
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç img600x866 –µ—Å–ª–∏ —ç—Ç–æ –æ–±—ã—á–Ω—ã–π CDN URL
                    if '/img600x866/' not in full_url and 'a.lmcdn.ru' in full_url:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑ URL
                        # –ù–∞–ø—Ä–∏–º–µ—Ä: https://a.lmcdn.ru/R/T/RTLAEF651001_27427936_1_v4_2x.jpg
                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤: https://a.lmcdn.ru/img600x866/R/T/RTLAEF651001_27427936_1_v4_2x.jpg
                        path_part = full_url.replace('https://a.lmcdn.ru/', '')
                        if path_part and not path_part.startswith('img600x866/'):
                            full_url = f"https://a.lmcdn.ru/img600x866/{path_part}"
                    
                    return full_url
                return None
            
            def walk_and_collect(obj):
                """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ –æ–±—ä–µ–∫—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
                if len(found_images) >= 8:
                    return
                
                if isinstance(obj, str):
                    # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if any(ext in obj.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                        normalized = normalize_image_url(obj)
                        if normalized and normalized not in found_images:
                            found_images.append(normalized)
                
                elif isinstance(obj, list):
                    for item_elem in obj:
                        walk_and_collect(item_elem)
                        if len(found_images) >= 8:
                            break
                
                elif isinstance(obj, dict):
                    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç URL
                    priority_keys = ['url', 'src', 'href', 'path', 'image_url']
                    for key in priority_keys:
                        if key in obj and len(found_images) < 8:
                            walk_and_collect(obj[key])
                    
                    # –ó–∞—Ç–µ–º –æ–±—Ö–æ–¥–∏–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è
                    for key, value in obj.items():
                        if key not in priority_keys and len(found_images) < 8:
                            walk_and_collect(value)
            
            # –ü–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_fields = [
                'image', 'images', 'photo', 'photos', 'picture', 'pictures',
                'main_image', 'preview_image', 'thumb', 'thumbs',
                'product_image', 'product_images', 'media', 'assets',
                'thumbnail', 'gallery'
            ]
            
            for field in image_fields:
                if field in item and len(found_images) < 8:
                    walk_and_collect(item[field])
            
            # –û—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ thumbnail
            thumbnail = item.get('thumbnail', '')
            if thumbnail:
                normalized = normalize_image_url(thumbnail)
                if normalized and normalized not in found_images:
                    found_images.insert(0, normalized)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ –∫–∞–∫ –≥–ª–∞–≤–Ω–æ–µ
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –≥–∞–ª–µ—Ä–µ–∏
            gallery = item.get('gallery', [])
            if isinstance(gallery, list):
                for img_path in gallery:
                    if len(found_images) >= 8:
                        break
                    normalized = normalize_image_url(img_path)
                    if normalized and normalized not in found_images:
                        found_images.append(normalized)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            product_data['image_urls'] = found_images
            product_data['image_url'] = found_images[0] if found_images else ""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
            if not product_data['sku'] or not product_data['name'] or not product_data['price']:
                return None
            
            return product_data
            
        except Exception as e:
            logger.error(f"Error parsing Lamoda product JSON: {e}")
            return None

    def _parse_product_from_element(self, element, index: int = 0) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–∞ –∏–∑ HTML —ç–ª–µ–º–µ–Ω—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            product_data = {
                'name': None,
                'brand': None,
                'price': None,
                'old_price': None,
                'url': None,
                'image_url': None,
                'image_urls': []
            }
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            name_selectors = [
                'h3[class*="title"]', 'div[class*="title"]', 'span[class*="title"]',
                '[data-testid*="title"]', '[data-testid*="name"]',
                'h1, h2, h3, h4', '.product-card__product-name'
            ]
            
            for selector in name_selectors:
                name_elem = element.select_one(selector)
                if name_elem:
                    name_text = name_elem.get_text(strip=True)
                    if name_text and len(name_text) > 3:
                        product_data['name'] = name_text[:100]
                        break
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–∞
            brand_selectors = [
                'span[class*="brand"]', 'div[class*="brand"]',
                '[data-testid*="brand"]', '.product-card__brand-name'
            ]
            
            for selector in brand_selectors:
                brand_elem = element.select_one(selector)
                if brand_elem:
                    brand_text = brand_elem.get_text(strip=True)
                    if brand_text and len(brand_text) > 1:
                        product_data['brand'] = brand_text
                        break
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω
            price_info = self._extract_prices_from_element(element)
            if price_info:
                product_data['price'] = price_info.get('current_price')
                product_data['old_price'] = price_info.get('old_price')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL —Ç–æ–≤–∞—Ä–∞
            url_selectors = ['a[href*="/p/"]', 'a[href]']
            for selector in url_selectors:
                link_elem = element.select_one(selector)
                if link_elem and link_elem.get('href'):
                    href = link_elem['href']
                    if href.startswith('/p/') or '/p/' in href:
                        if href.startswith('/'):
                            product_data['url'] = urljoin(self.base_url, href)
                        elif href.startswith('http'):
                            product_data['url'] = href
                        break
            
            # –£–õ–£–ß–®–ï–ù–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–∏–∑ —Ä–∞–±–æ—á–µ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞)
            found_images = set()
            
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π Lamoda
            img_selectors = [
                'img[src*="lmcdn"]', 'img[data-src*="lmcdn"]',
                'img[data-lazy-src*="lmcdn"]', 'img[data-original*="lmcdn"]',
                'img[class*="image"]', 'img[class*="picture"]', 'img'
            ]
            
            for selector in img_selectors:
                img_elems = element.select(selector)
                for img in img_elems:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    src_attrs = ['src', 'data-src', 'data-lazy-src', 'data-original', 'data-srcset', 'srcset']
                    src = None
                    
                    for attr in src_attrs:
                        raw_val = img.get(attr)
                        if not raw_val:
                            continue
                        # –î–ª—è –∞—Ç—Ä–∏–±—É—Ç–æ–≤ srcset / data-srcset –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π URL –¥–æ –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ –ø—Ä–æ–±–µ–ª–∞
                        if attr in ['srcset', 'data-srcset']:
                            # srcset —Ñ–æ—Ä–º–∞—Ç–∞: "url1 1x, url2 2x" –∏–ª–∏ "url1 236w"
                            raw_val = raw_val.split(',')[0].strip().split(' ')[0].strip()
                        src = raw_val
                        break
                    
                    if not src:
                        continue
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–º –ø–∞—Ä—Å–µ—Ä–µ)
                    if src.startswith('//'):
                        full_url = 'https:' + src
                    elif src.startswith('/'):
                        full_url = urljoin(self.base_url, src)
                    else:
                        full_url = src
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è Lamoda
                    if (full_url and 
                        ('lmcdn.ru' in full_url or 'lamoda' in full_url) and
                        any(ext in full_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp'])):
                        found_images.add(full_url)
                
                if len(found_images) >= 8:
                    break
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ inline-—Å—Ç–∏–ª—è—Ö background-image
            if len(found_images) < 8:
                styled_elems = element.select('[style*="background-image"], [data-bg], [data-image]')
                for styled in styled_elems:
                    style_attr = styled.get('style', '')
                    candidates = re.findall(r'url\(([^)]+)\)', style_attr)
                    for c in candidates:
                        if len(found_images) >= 8:
                            break
                        # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏ –ø—Ä–æ–±–µ–ª—ã
                        c = c.strip('"\'').strip()
                        if not c:
                            continue
                        if c.startswith('//'):
                            full_url = 'https:' + c
                        elif c.startswith('/'):
                            full_url = urljoin(self.base_url, c)
                        else:
                            full_url = c
                        
                        if (full_url and 
                            ('lmcdn.ru' in full_url or 'lamoda' in full_url) and
                            any(ext in full_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']) and
                            full_url not in found_images):
                            found_images.add(full_url)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            product_data['image_urls'] = list(found_images)
            if product_data['image_urls']:
                product_data['image_url'] = product_data['image_urls'][0]
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º SKU
            sku = f"LMD{index + 1:04d}"
            if product_data['url']:
                sku_match = re.search(r'/([A-Z0-9]+)/?(?:\?|$)', product_data['url'])
                if sku_match:
                    sku = sku_match.group(1)
            
            product_data['sku'] = sku
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            if not product_data['name']:
                product_data['name'] = "Product"
            if not product_data['brand']:
                product_data['brand'] = "Unknown"
            if not product_data['image_url']:
                product_data['image_url'] = ""
            
            return product_data
            
        except Exception as e:
            logger.error(f"Error parsing product from element {index}: {e}")
            return None

    def _parse_product_card(self, card, index: int) -> Optional[ParsedProduct]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ –º–µ—Ç–æ–¥, —á—Ç–æ –∏ –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        return self._parse_product_from_element(card, index)

    def _extract_prices_from_element(self, element) -> Optional[Dict[str, Optional[float]]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            price_info = {'current_price': None, 'old_price': None}
            
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ü–µ–Ω
            price_selectors = {
                'current': [
                    '[data-testid*="price-current"]',
                    '.price-current', '.price__current',
                    'span[class*="price"]:not([class*="old"])',
                    '.product-price__current'
                ],
                'old': [
                    '[data-testid*="price-old"]',
                    '.price-old', '.price__old',
                    'span[class*="price"][class*="old"]',
                    '.product-price__old'
                ]
            }
            
            # –ò—â–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—É—é —Ü–µ–Ω—É
            for selector in price_selectors['current']:
                elem = element.select_one(selector)
                if elem:
                    price_text = elem.get_text(strip=True)
                    if price_text and (self.currency in price_text):
                        price = self._extract_price_from_text(price_text)
                        if price:
                            price_info['current_price'] = price
                            break
            
            # –ò—â–µ–º —Å—Ç–∞—Ä—É—é —Ü–µ–Ω—É
            for selector in price_selectors['old']:
                elem = element.select_one(selector)
                if elem:
                    price_text = elem.get_text(strip=True)
                    if price_text and (self.currency in price_text):
                        price = self._extract_price_from_text(price_text)
                        if price:
                            price_info['old_price'] = price
                            break
            
            # Fallback: –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —ç–ª–µ–º–µ–Ω—Ç–∞
            if not price_info['current_price']:
                element_text = element.get_text()
                # –ò–∑–º–µ–Ω–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ü–µ–Ω —Å –≤–µ–¥—É—â–∏–º–∏ –Ω—É–ª—è–º–∏
                price_matches = re.findall(rf'(\d+(?:\s+\d+)*)\s*{re.escape(self.currency)}', element_text)
                
                if price_matches:
                    prices = []
                    for price_str in price_matches:
                        try:
                            # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏
                            clean_price = price_str.replace(' ', '').lstrip('0') or '0'
                            price = float(clean_price)
                            if 100 <= price <= 10000000:
                                prices.append(price)
                        except ValueError:
                            continue
                    
                    if prices:
                        prices.sort()
                        price_info['current_price'] = prices[0]
                        if len(prices) > 1:
                            price_info['old_price'] = prices[-1]
            
            return price_info if price_info['current_price'] else None
            
        except Exception as e:
            logger.debug(f"Price extraction error: {e}")
            return None

    def _extract_price_from_text(self, text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return None
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        clean_text = text.replace(self.currency, '').replace('‚Ç∏', '').replace('‚ÇΩ', '').replace('—Ä.', '').strip()
        
        # –ò—â–µ–º —Ü–µ–Ω—ã —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ (–≤–∫–ª—é—á–∞—è –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏)
        price_pattern = r'\b(\d+(?:\s+\d+)*)\b'
        matches = re.findall(price_pattern, clean_text)
        
        if matches:
            for match in matches:
                try:
                    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å–ª–æ
                    price_str = match.replace(' ', '')
                    # –£–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–µ –Ω—É–ª–∏
                    price = float(price_str.lstrip('0') or '0')
                    if 100 <= price <= 10000000:
                        return price
                except ValueError:
                    continue
        
        # Fallback: —á–∏—Å–ª–∞ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤
        fallback_matches = re.findall(r'\b(\d{3,7})\b', clean_text)
        if fallback_matches:
            for match in fallback_matches:
                try:
                    price = float(match)
                    if 100 <= price <= 10000000:
                        return price
                except ValueError:
                    continue
        
        return None

    def _normalize_brand(self, brand: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –±—Ä–µ–Ω–¥–∞"""
        if not brand or brand.lower() in ['unknown', 'none', '']:
            return "Unknown"
        
        # –ò—â–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –±—Ä–µ–Ω–¥—ã
        brand_lower = brand.lower()
        for known_brand in KNOWN_BRANDS:
            if known_brand.lower() == brand_lower:
                return known_brand
            if known_brand.lower() in brand_lower:
                return known_brand
        
        # –û—á–∏—â–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
        cleaned = re.sub(r'[^\w\s&\.]', '', brand).strip()
        first_word = cleaned.split()[0] if cleaned.split() else brand
        return first_word.title()

    def _extract_brand_from_name(self, name: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        if not name:
            return "Unknown"
        
        # –ò—â–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –±—Ä–µ–Ω–¥—ã –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        name_upper = name.upper()
        for brand in KNOWN_BRANDS:
            if brand.upper() in name_upper:
                return brand
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –±—Ä–µ–Ω–¥
        words = name.split()
        if words:
            first_word = words[0]
            if len(first_word) > 2 and first_word.isalpha():
                return first_word.title()
        
        return "Unknown"

    def _extract_clothing_type(self, name: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –æ–¥–µ–∂–¥—ã –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –ø–æ–¥ –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –∏–∑ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        if not name:
            return None
        
        name_lower = name.lower()
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞ 5 —Å—Ç—Ä–æ–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –æ–±—Ä–∞–∑–∞
        clothing_patterns = {
            # –í–°–Å –î–õ–Ø –í–ï–†–•–ê -> top
            'top': [
                '—Ñ—É—Ç–±–æ–ª–∫–∞', '—Ñ—É—Ç–±–æ–ª–∫–∏', 't-shirt', 'tshirt', '–º–∞–π–∫–∞', '–º–∞–π–∫–∏',
                '—Ä—É–±–∞—à–∫–∞', '—Ä—É–±–∞—à–∫–∏', '–±–ª—É–∑–∫–∞', '–±–ª—É–∑–∫–∏', '–±–ª—É–∑–∞', '—Å–æ—Ä–æ—á–∫–∞', 'shirt',
                '—Ö—É–¥–∏', '—Ç–æ–ª—Å—Ç–æ–≤–∫–∞', '—Ç–æ–ª—Å—Ç–æ–≤–∫–∏', 'hoodie', '—Å–≤–∏—Ç—à–æ—Ç', '—Å–≤–∏—Ç—à–æ—Ç—ã',
                '—Å–≤–∏—Ç–µ—Ä', '—Å–≤–∏—Ç–µ—Ä—ã', '–¥–∂–µ–º–ø–µ—Ä', '–¥–∂–µ–º–ø–µ—Ä—ã', '–ø—É–ª–æ–≤–µ—Ä', '–ø—É–ª–æ–≤–µ—Ä—ã', '–∫–∞—Ä–¥–∏–≥–∞–Ω', '–∫–∞—Ä–¥–∏–≥–∞–Ω—ã',
                '–∫—É—Ä—Ç–∫–∞', '–∫—É—Ä—Ç–∫–∏', '–∂–∞–∫–µ—Ç', '–∂–∞–∫–µ—Ç—ã', '–ø–∏–¥–∂–∞–∫', '–ø–∏–¥–∂–∞–∫–∏', '–±–æ–º–±–µ—Ä', '–≤–µ—Ç—Ä–æ–≤–∫–∞',
                '–ø–∞–ª—å—Ç–æ', '—à—É–±–∞', '—à—É–±—ã', '–ø–ª–∞—â', '–ø–ª–∞—â–∏', '—Ç—Ä–µ–Ω—á', '–ø–∞—Ä–∫–∞', '–ø–∞—Ä–∫–∏',
                '–ø–ª–∞—Ç—å–µ', '–ø–ª–∞—Ç—å—è', '—Å–∞—Ä–∞—Ñ–∞–Ω', '—Å–∞—Ä–∞—Ñ–∞–Ω—ã', 'dress', '–∫–æ—Ñ—Ç–∞', '–∫–æ—Ñ—Ç—ã',
                '–ª–æ–Ω–≥—Å–ª–∏–≤', '–ª–æ–Ω–≥—Å–ª–∏–≤—ã', 'longsleeve', '–ø–æ–ª–æ', 'polo',
                'sweater', 'jacket', 'coat', 'blouse'
            ],
            
            # –í–°–Å –î–õ–Ø –ù–ò–ó–ê -> bottom
            'bottom': [
                '–±—Ä—é–∫–∏', '—à—Ç–∞–Ω—ã', '–ª–µ–≥–≥–∏–Ω—Å—ã', '–ª–æ—Å–∏–Ω—ã', 'pants', 'trousers',
                '–¥–∂–∏–Ω—Å—ã', 'jeans', 'denim', '—à–æ—Ä—Ç—ã', 'shorts',
                '—é–±–∫–∞', '—é–±–∫–∏', 'skirt', '–∫–∞–ø—Ä–∏', '–ª–µ–≥–∏–Ω—Å—ã'
            ],
            
            # –û–ë–£–í–¨ -> footwear
            'footwear': [
                '–∫—Ä–æ—Å—Å–æ–≤–∫–∏', '–±–æ—Ç–∏–Ω–∫–∏', '—Ç—É—Ñ–ª–∏', '—Å–∞–ø–æ–≥–∏', '–±–æ—Å–æ–Ω–æ–∂–∫–∏', '—Å–∞–Ω–¥–∞–ª–∏–∏', 
                '–∫–µ–¥—ã', '–º–æ–∫–∞—Å–∏–Ω—ã', '–ª–æ—Ñ–µ—Ä—ã', '–æ–±—É–≤—å', 'shoes', 'sneakers', 'boots',
                '–±–∞–ª–µ—Ç–∫–∏', '—Å–ª–∏–ø–æ–Ω—ã', '—É–≥–≥–∏'
            ],
            
            # –ê–ö–°–ï–°–°–£–ê–†–´ -> accessory
            'accessory': [
                '—Å—É–º–∫–∞', '—Å—É–º–∫–∏', '—Ä—é–∫–∑–∞–∫', '—Ä—é–∫–∑–∞–∫–∏', '—Ä–µ–º–µ–Ω—å', '—Ä–µ–º–Ω–∏', '—à–∞—Ä—Ñ', '—à–∞—Ä—Ñ—ã',
                '–ø–ª–∞—Ç–æ–∫', '–ø–ª–∞—Ç–∫–∏', '–æ—á–∫–∏', '—á–∞—Å—ã', '—É–∫—Ä–∞—à–µ–Ω–∏—è', '–∫–æ–ª—å—Ü–æ', '—Å–µ—Ä—å–≥–∏',
                '–±—Ä–∞—Å–ª–µ—Ç', '—Ü–µ–ø–æ—á–∫–∞', '–∫—É–ª–æ–Ω', '–ø–µ—Ä—á–∞—Ç–∫–∏', '–≤–∞—Ä–µ–∂–∫–∏', '—à–∞–ø–∫–∞', '—à–∞–ø–∫–∏',
                '–∫–µ–ø–∫–∞', '–∫–µ–ø–∫–∏', '–±–µ–π—Å–±–æ–ª–∫–∞', '–ø–∞–Ω–∞–º–∞', '–±–µ—Ä–µ—Ç', 'bag', 'watch'
            ],
            
            # –ê–†–û–ú–ê–¢–´ -> fragrance
            'fragrance': [
                '–¥—É—Ö–∏', '–ø–∞—Ä—Ñ—é–º', '—Ç—É–∞–ª–µ—Ç–Ω–∞—è –≤–æ–¥–∞', '–æ–¥–µ–∫–æ–ª–æ–Ω', '–∞—Ä–æ–º–∞—Ç', 'fragrance',
                'perfume', 'eau de toilette', 'eau de parfum', '–¥–µ–∑–æ–¥–æ—Ä–∞–Ω—Ç', '–ø–∞—Ä—Ñ—é–º–µ—Ä–∏—è',
                '–º–∞—Å–ª–æ —ç—Ñ–∏—Ä–Ω–æ–µ', '—ç—Ñ–∏—Ä–Ω–æ–µ –º–∞—Å–ª–æ', '—Å–ø—Ä–µ–π –∞—Ä–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π', '–ø–∞—Ä—Ñ—é–º–µ—Ä–Ω–∞—è –≤–æ–¥–∞'
            ]
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        for clothing_type, keywords in clothing_patterns.items():
            for keyword in keywords:
                if keyword in name_lower:
                    return clothing_type
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if any(word in name_lower for word in ['–º—É–∂—Å–∫', '–∂–µ–Ω—Å–∫', '–¥–µ—Ç—Å–∫']):
            # –ï—Å–ª–∏ –µ—Å—Ç—å —É–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –ø–æ–ª/–≤–æ–∑—Ä–∞—Å—Ç, –ø—ã—Ç–∞–µ–º—Å—è –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            if any(word in name_lower for word in ['–≤–µ—Ä—Ö', '—Ç–æ–ø']):
                return 'top'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –≤–µ—Ä—Ö–∞
            elif any(word in name_lower for word in ['–Ω–∏–∑', 'bottom']):
                return 'bottom'   # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –Ω–∏–∑–∞
        
        return None

    def _normalize_category_for_outfits(self, category: Optional[str], clothing_type: Optional[str], name: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –Ω–æ–≤–æ–π —Å–∏—Å—Ç–µ–º–æ–π –æ–±—Ä–∞–∑–æ–≤
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–Ω—É –∏–∑ 5 —Å—Ç—Ä–æ–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:
        - top (–≤–µ—Ä—Ö) - —Ñ—É—Ç–±–æ–ª–∫–∏, —Ä—É–±–∞—à–∫–∏, –ø–ª–∞—Ç—å—è, –∫—É—Ä—Ç–∫–∏, —Å–≤–∏—Ç–µ—Ä–∞
        - bottom (–Ω–∏–∑) - –¥–∂–∏–Ω—Å—ã, —é–±–∫–∏, —à–æ—Ä—Ç—ã, –±—Ä—é–∫–∏, –ª–µ–≥–≥–∏–Ω—Å—ã
        - footwear (–æ–±—É–≤—å) - –∫—Ä–æ—Å—Å–æ–≤–∫–∏, —Ç—É—Ñ–ª–∏, –±–æ—Ç–∏–Ω–∫–∏, —Å–∞–Ω–¥–∞–ª–∏–∏
        - accessory (–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã) - —Å—É–º–∫–∏, —á–∞—Å—ã, –æ—á–∫–∏, —É–∫—Ä–∞—à–µ–Ω–∏—è  
        - fragrance (–∞—Ä–æ–º–∞—Ç—ã) - –¥—É—Ö–∏, –ø–∞—Ä—Ñ—é–º, —Ç—É–∞–ª–µ—Ç–Ω–∞—è –≤–æ–¥–∞
        """
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å clothing_type –µ—Å–ª–∏ –æ–Ω —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω
        if clothing_type:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç–∞—Ä—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –Ω–æ–≤—ã–µ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            old_to_new_mapping = {
                # –í—Å–µ –≤–∏–¥—ã –≤–µ—Ä—Ö–∞ -> top
                'tshirt': 'top', 'shirt': 'top', 'hoodie': 'top', 'sweater': 'top', 
                'jacket': 'top', 'coat': 'top', 'dress': 'top',
                # –í—Å–µ –≤–∏–¥—ã –Ω–∏–∑–∞ -> bottom  
                'pants': 'bottom', 'jeans': 'bottom', 'shorts': 'bottom', 'skirt': 'bottom',
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å
                'footwear': 'footwear', 'accessories': 'accessory', 'fragrances': 'fragrance'
            }
            return old_to_new_mapping.get(clothing_type, clothing_type)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å category, –ø—ã—Ç–∞–µ–º—Å—è –µ–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å
        if category:
            category_lower = category.lower()
            
            # –ü—Ä—è–º–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å –Ω–æ–≤—ã–º–∏ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
            if category_lower in ['top', 'bottom', 'footwear', 'accessory', 'fragrance']:
                return category_lower
            
            # –ú–∞–ø–ø–∏–Ω–≥ –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –Ω–∞ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            category_mapping = {
                # –í–°–Å –î–õ–Ø –í–ï–†–•–ê -> top
                'tops': 'top', '–≤–µ—Ä—Ö': 'top', '—Ñ—É—Ç–±–æ–ª–∫–∞': 'top', '–º–∞–π–∫–∞': 'top',
                '—Ä—É–±–∞—à–∫–∞': 'top', '–±–ª—É–∑–∫–∞': 'top', '–±–ª—É–∑–∞': 'top', 'shirt': 'top',
                '—Ç–æ–ª—Å—Ç–æ–≤–∫–∞': 'top', '—Ö—É–¥–∏': 'top', '—Å–≤–∏—Ç—à–æ—Ç': 'top', 'hoodie': 'top',
                '—Å–≤–∏—Ç–µ—Ä': 'top', '–¥–∂–µ–º–ø–µ—Ä': 'top', '–ø—É–ª–æ–≤–µ—Ä': 'top', 'sweater': 'top',
                '–∫—É—Ä—Ç–∫–∞': 'top', '–∂–∞–∫–µ—Ç': 'top', '–ø–∏–¥–∂–∞–∫': 'top', 'jacket': 'top',
                '–ø–∞–ª—å—Ç–æ': 'top', '–ø–ª–∞—â': 'top', 'coat': 'top',
                '–ø–ª–∞—Ç—å–µ': 'top', '—Å–∞—Ä–∞—Ñ–∞–Ω': 'top', 'dress': 'top',
                '–∫–æ—Ñ—Ç–∞': 'top', '–∫–æ—Ñ—Ç—ã': 'top', '–¥–∂–µ—Ä—Å–∏': 'top', 'tshirt': 'top',
                
                # –í–°–Å –î–õ–Ø –ù–ò–ó–ê -> bottom  
                'bottoms': 'bottom', '–Ω–∏–∑': 'bottom', '–±—Ä—é–∫–∏': 'bottom', '—à—Ç–∞–Ω—ã': 'bottom',
                '–ª–µ–≥–≥–∏–Ω—Å—ã': 'bottom', '–ª–µ–≥–∏–Ω—Å—ã': 'bottom', 'pants': 'bottom', 'trousers': 'bottom',
                '–¥–∂–∏–Ω—Å—ã': 'bottom', 'jeans': 'bottom', 'denim': 'bottom',
                '—à–æ—Ä—Ç—ã': 'bottom', 'shorts': 'bottom',
                '—é–±–∫–∞': 'bottom', '—é–±–∫–∏': 'bottom', 'skirt': 'bottom',
                '–ª–æ—Å–∏–Ω—ã': 'bottom', '–∫–∞–ø—Ä–∏': 'bottom',
                
                # –û–ë–£–í–¨ -> footwear
                '–æ–±—É–≤—å': 'footwear', 'shoes': 'footwear', '–∫—Ä–æ—Å—Å–æ–≤–∫–∏': 'footwear',
                '–±–æ—Ç–∏–Ω–∫–∏': 'footwear', '—Ç—É—Ñ–ª–∏': 'footwear', '—Å–∞–ø–æ–≥–∏': 'footwear',
                '–±–æ—Å–æ–Ω–æ–∂–∫–∏': 'footwear', '—Å–∞–Ω–¥–∞–ª–∏–∏': 'footwear', '–±–∞–ª–µ—Ç–∫–∏': 'footwear',
                '–∫–µ–¥—ã': 'footwear', '—Å–ª–∏–ø–æ–Ω—ã': 'footwear', '–º–æ–∫–∞—Å–∏–Ω—ã': 'footwear',
                'boots': 'footwear', 'sneakers': 'footwear', 'sandals': 'footwear',
                
                # –ê–ö–°–ï–°–°–£–ê–†–´ -> accessory
                '–∞–∫—Å–µ—Å—Å—É–∞—Ä—ã': 'accessory', 'accessories': 'accessory', '—Å—É–º–∫–∞': 'accessory',
                '—Å—É–º–∫–∏': 'accessory', '—Ä—é–∫–∑–∞–∫': 'accessory', '—Ä—é–∫–∑–∞–∫–∏': 'accessory',
                '—Ä–µ–º–µ–Ω—å': 'accessory', '—Ä–µ–º–Ω–∏': 'accessory', '—á–∞—Å—ã': 'accessory',
                '–æ—á–∫–∏': 'accessory', '—É–∫—Ä–∞—à–µ–Ω–∏—è': 'accessory', '–∫–æ–ª—å—Ü–æ': 'accessory',
                '—Å–µ—Ä—å–≥–∏': 'accessory', '–±—Ä–∞—Å–ª–µ—Ç': 'accessory', '—à–∞—Ä—Ñ': 'accessory',
                '–ø–ª–∞—Ç–æ–∫': 'accessory', '–ø–µ—Ä—á–∞—Ç–∫–∏': 'accessory', '—à–∞–ø–∫–∞': 'accessory',
                'bag': 'accessory', 'watch': 'accessory', 'glasses': 'accessory',
                
                # –ê–†–û–ú–ê–¢–´ -> fragrance  
                '–¥—É—Ö–∏': 'fragrance', '–ø–∞—Ä—Ñ—é–º': 'fragrance', '–∞—Ä–æ–º–∞—Ç': 'fragrance',
                '—Ç—É–∞–ª–µ—Ç–Ω–∞—è –≤–æ–¥–∞': 'fragrance', '–æ–¥–µ–∫–æ–ª–æ–Ω': 'fragrance', '–ø–∞—Ä—Ñ—é–º–µ—Ä–∏—è': 'fragrance',
                'fragrance': 'fragrance', 'perfume': 'fragrance', 'cologne': 'fragrance'
            }
            
            if category_lower in category_mapping:
                return category_mapping[category_lower]
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ, –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
        detected_type = self._smart_extract_category_from_name(name)
        return detected_type or 'accessory'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∞–∫—Å–µ—Å—Å—É–∞—Ä—ã
    
    def _smart_extract_category_from_name(self, name: str) -> Optional[str]:
        """–£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –∏–∑ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞"""
        if not name:
            return None
        
        name_lower = name.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_keywords = {
            'top': [
                '—Ñ—É—Ç–±–æ–ª–∫–∞', '–º–∞–π–∫–∞', '—Ä—É–±–∞—à–∫–∞', '–±–ª—É–∑–∫–∞', '–±–ª—É–∑–∞', '–ø–ª–∞—Ç—å–µ',
                '—Ö—É–¥–∏', '—Ç–æ–ª—Å—Ç–æ–≤–∫–∞', '—Å–≤–∏—Ç—à–æ—Ç', '—Å–≤–∏—Ç–µ—Ä', '–¥–∂–µ–º–ø–µ—Ä', '–ø—É–ª–æ–≤–µ—Ä',
                '–∫—É—Ä—Ç–∫–∞', '–∂–∞–∫–µ—Ç', '–ø–∏–¥–∂–∞–∫', '–ø–∞–ª—å—Ç–æ', '–ø–ª–∞—â', '–∫–æ—Ñ—Ç–∞',
                '–ª–æ–Ω–≥—Å–ª–∏–≤', '–ª–æ–Ω–≥—Å–ª–∏–≤—ã', '–ø–æ–ª–æ',
                't-shirt', 'tshirt', 'shirt', 'blouse', 'dress', 'hoodie',
                'sweater', 'jacket', 'coat', 'cardigan', 'longsleeve', 'polo'
            ],
            'bottom': [
                '–±—Ä—é–∫–∏', '—à—Ç–∞–Ω—ã', '–¥–∂–∏–Ω—Å—ã', '—à–æ—Ä—Ç—ã', '—é–±–∫–∞', '–ª–µ–≥–≥–∏–Ω—Å—ã',
                '–ª–æ—Å–∏–Ω—ã', '–∫–∞–ø—Ä–∏', 'pants', 'jeans', 'shorts', 'skirt',
                'trousers', 'leggings'
            ],
            'footwear': [
                '–∫—Ä–æ—Å—Å–æ–≤–∫–∏', '–±–æ—Ç–∏–Ω–∫–∏', '—Ç—É—Ñ–ª–∏', '—Å–∞–ø–æ–≥–∏', '–±–æ—Å–æ–Ω–æ–∂–∫–∏',
                '—Å–∞–Ω–¥–∞–ª–∏–∏', '–±–∞–ª–µ—Ç–∫–∏', '–∫–µ–¥—ã', '–º–æ–∫–∞—Å–∏–Ω—ã', '–æ–±—É–≤—å',
                'sneakers', 'boots', 'shoes', 'sandals', 'flats'
            ],
            'accessory': [
                '—Å—É–º–∫–∞', '—Ä—é–∫–∑–∞–∫', '—Ä–µ–º–µ–Ω—å', '—á–∞—Å—ã', '–æ—á–∫–∏', '—à–∞—Ä—Ñ',
                '–ø–ª–∞—Ç–æ–∫', '–ø–µ—Ä—á–∞—Ç–∫–∏', '—à–∞–ø–∫–∞', '–∫–µ–ø–∫–∞', '—É–∫—Ä–∞—à–µ–Ω–∏—è',
                'bag', 'backpack', 'belt', 'watch', 'glasses', 'scarf'
            ],
            'fragrance': [
                '–¥—É—Ö–∏', '–ø–∞—Ä—Ñ—é–º', '—Ç—É–∞–ª–µ—Ç–Ω–∞—è –≤–æ–¥–∞', '–æ–¥–µ–∫–æ–ª–æ–Ω', '–∞—Ä–æ–º–∞—Ç',
                '–º–∞—Å–ª–æ —ç—Ñ–∏—Ä–Ω–æ–µ', '—ç—Ñ–∏—Ä–Ω–æ–µ', '—Å–ø—Ä–µ–π –∞—Ä–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π', '–ø–∞—Ä—Ñ—é–º–µ—Ä–Ω–∞—è –≤–æ–¥–∞',
                'perfume', 'fragrance', 'cologne', 'eau de toilette', 'essential oil'
            ]
        }
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        for category, keywords in category_keywords.items():
            for keyword in keywords:
                if keyword in name_lower:
                    return category
        
        return None

    def _extract_sku_from_url(self, url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ SKU –∏–∑ URL"""
        try:
            path_parts = urlparse(url).path.strip('/').split('/')
            
            # –ò—â–µ–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ /p/
            if len(path_parts) >= 2 and path_parts[0] == 'p':
                sku_candidate = path_parts[1]
                if len(sku_candidate) >= 8 and sku_candidate.replace('-', '').isalnum():
                    return sku_candidate.upper()
            
            # –ò—â–µ–º –ª—é–±—É—é –¥–ª–∏–Ω–Ω—É—é –∞–ª—Ñ–∞–≤–∏—Ç–Ω–æ-—Ü–∏—Ñ—Ä–æ–≤—É—é —á–∞—Å—Ç—å
            for part in path_parts:
                if len(part) >= 8 and part.replace('-', '').replace('_', '').isalnum():
                    return part.upper()
            
            return None
            
        except Exception:
            return None

    def _deduplicate_and_enhance(self, products: List[ParsedProduct]) -> List[ParsedProduct]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        seen_skus = set()
        seen_names = set()
        unique_products = []
        
        for product in products:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ SKU
            if product.sku in seen_skus:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é (–¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤)
            name_key = f"{product.brand}:{product.name}".lower()
            if name_key in seen_names:
                continue
            
            # –£–ª—É—á—à–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            enhanced_product = self._enhance_product_data(product)
            
            seen_skus.add(product.sku)
            seen_names.add(name_key)
            unique_products.append(enhanced_product)
        
        return unique_products

    def _enhance_product_data(self, product: ParsedProduct) -> ParsedProduct:
        """–£–ª—É—á—à–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞"""
        # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ–ª–µ–π
        quality_factors = {
            'has_sku': 0.1 if product.sku else 0,
            'has_name': 0.2 if product.name else 0,
            'has_brand': 0.15 if product.brand != "Unknown" else 0,
            'has_price': 0.2 if product.price > 0 else 0,
            'has_url': 0.1 if product.url else 0,
            'has_image': 0.1 if product.image_url else 0,
            'has_description': 0.05 if product.description else 0,
            'has_clothing_type': 0.05 if product.clothing_type else 0,
            'has_multiple_images': 0.05 if len(product.image_urls) > 1 else 0
        }
        
        base_quality = product.parse_quality
        enhancement_factor = sum(quality_factors.values())
        product.parse_quality = min(1.0, base_quality + enhancement_factor)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        product.parse_metadata.update({
            'enhanced': True,
            'quality_factors': quality_factors,
            'enhancement_factor': enhancement_factor
        })
        
        return product

    def _calculate_quality_score(self, products: List[ParsedProduct]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if not products:
            return 0.0
        
        total_quality = sum(p.parse_quality for p in products)
        avg_quality = total_quality / len(products)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤
        quantity_bonus = min(0.1, len(products) / 100)
        
        return min(1.0, avg_quality + quantity_bonus)

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if self.session:
            await self.session.aclose()
            self.session = None
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à
        self.cache.clear()
        
        logger.info(f"Parser closed. Stats: {self.stats}")

    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä"""
        if self.session:
            try:
                asyncio.create_task(self.close())
            except:
                pass

    def _extract_json_products(self, soup: BeautifulSoup) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ JSON –¥–∞–Ω–Ω—ã—Ö –≤ —Å–∫—Ä–∏–ø—Ç–∞—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        products = []
        
        def _find_products_array(text: str) -> Optional[str]:
            """–ù–∞–π—Ç–∏ –∏ –≤–µ—Ä–Ω—É—Ç—å JSON-—Å—Ç—Ä–æ–∫—É –º–∞—Å—Å–∏–≤–∞ products —Å –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π —Å–∫–æ–±–æ–∫"""
            key = '"products"'
            start_key = text.find(key)
            if start_key == -1:
                return None
            # –∏—â–µ–º –ø–µ—Ä–≤—É—é –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é –∫–≤–∞–¥—Ä–∞—Ç–Ω—É—é —Å–∫–æ–±–∫—É –ø–æ—Å–ª–µ –∫–ª—é—á–∞
            array_start = text.find('[', start_key)
            if array_start == -1:
                return None
            depth = 0
            for idx in range(array_start, len(text)):
                ch = text[idx]
                if ch == '[':
                    depth += 1
                elif ch == ']':
                    depth -= 1
                    if depth == 0:
                        return text[array_start:idx + 1]
            return None
        
        try:
            script_tags = soup.find_all('script')
            
            for script in script_tags:
                if not script.string:
                    continue
                    
                script_content = script.string.strip()

                # 1) –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ products —á–µ—Ä–µ–∑ helper (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ __NUXT__)
                if '"products"' in script_content:
                    products_json_str = _find_products_array(script_content)
                    if products_json_str:
                        try:
                            products_data = json.loads(products_json_str)
                            if isinstance(products_data, list):
                                products.extend(products_data)
                                logger.info(f"Found {len(products_data)} products in JSON")
                                return products
                        except json.JSONDecodeError as e:
                            logger.debug(f"JSON decode error (products array): {e}")
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ JSON —Å —Ç–æ–≤–∞—Ä–∞–º–∏
                json_patterns = [
                    r'"products"\s*:\s*(\[[\s\S]*?\])',
                    r'window\.__INITIAL_STATE__\s*=\s*({[\s\S]*?});',
                    r'window\.dataLayer\s*=\s*(\[[\s\S]*?\]);',
                    r'window\.__NEXT_DATA__\s*=\s*({[\s\S]*?});',
                    r'catalogsearch.*?"products"\s*:\s*(\[[\s\S]*?\])',
                    r'"catalog"\s*:\s*{[\s\S]*?"items"\s*:\s*(\[[\s\S]*?\])',
                ]
                
                for pattern in json_patterns:
                    matches = re.findall(pattern, script_content, re.DOTALL)
                    
                    for match in matches:
                        try:
                            # –ï—Å–ª–∏ —ç—Ç–æ –º–∞—Å—Å–∏–≤ —Ç–æ–≤–∞—Ä–æ–≤
                            if match.strip().startswith('['):
                                products_data = json.loads(match)
                                if isinstance(products_data, list):
                                    products.extend(products_data)
                                    
                            # –ï—Å–ª–∏ —ç—Ç–æ –æ–±—ä–µ–∫—Ç —Å —Ç–æ–≤–∞—Ä–∞–º–∏
                            else:
                                data = json.loads(match)
                                found_products = self._find_products_in_object(data)
                                products.extend(found_products)
                                
                        except json.JSONDecodeError:
                            continue
                        except Exception:
                            continue
                
                if products:
                    break
        
        except Exception as e:
            logger.error(f"Error extracting JSON from scripts: {e}")
        
        return products

    def _find_products_in_object(self, data: dict) -> List[Dict[str, Any]]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤ JSON –æ–±—ä–µ–∫—Ç–µ"""
        products = []
        
        def search_recursive(obj, path=""):
            nonlocal products
                
            if isinstance(obj, dict):
                # –ò—â–µ–º –∫–ª—é—á–∏ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–≤–∞—Ä—ã
                product_keys = ['products', 'items', 'catalog', 'results', 'data']
                for key in product_keys:
                    if key in obj and isinstance(obj[key], list):
                        products.extend(obj[key])
                        return
                
                # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥—Ä—É–≥–∏–µ –∫–ª—é—á–∏
                for key, value in obj.items():
                    if key not in product_keys:
                        search_recursive(value, f"{path}.{key}")
                        
            elif isinstance(obj, list):
                for i, item in enumerate(obj):
                    search_recursive(item, f"{path}[{i}]")
        
        search_recursive(data)
        return products

    def _find_product_elements(self, soup: BeautifulSoup) -> List:
        """–ù–∞–π—Ç–∏ –±–ª–æ–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ –≤ HTML"""
        # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã, –Ω–∞—á–∏–Ω–∞—è —Å –Ω–∞–∏–±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö
        selectors = [
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ Lamoda
            'a[href*="/p/"]',  # –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã
            'div[class*="product"] a[href]',  # –°—Å—ã–ª–∫–∏ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤
            'article a[href]',  # –°—Å—ã–ª–∫–∏ –≤ article —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
            '.product-card a[href]',  # –°—Ç–∞—Ä—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            '.product-card',
            '.product-item',
            '.catalog-item',
            '.item-card',
            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∞—Å—Å–æ–≤
            '[class*="product"]',
            '[class*="item"]',
            '[class*="card"]',
            # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            'article',
            '.grid-item',
            'li[class*="item"]',
            # Fallback —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            'div[data-*]',
        ]
        
        for selector in selectors:
            blocks = soup.select(selector)
            if blocks and len(blocks) > 3:  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ö–æ—Ç—è –±—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–≤–∞—Ä–æ–≤
                logger.info(f"Found {len(blocks)} product blocks with selector: {selector}")
                return blocks
        
        return []

    async def _download_image(self, image_url: str, product_sku: str) -> Optional[str]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            if not image_url or not product_sku:
                return None
                
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            file_extension = self._get_file_extension(image_url)
            filename = f"{product_sku}_{int(time.time())}{file_extension}"
            filepath = self.images_dir / filename
            
            # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            async with httpx.AsyncClient() as client:
                response = await client.get(image_url, headers=self.headers, timeout=30.0)
                if response.status_code == 200:
                    async with aiofiles.open(filepath, 'wb') as f:
                        await f.write(response.content)
                    
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
                    return f"/uploads/items/{filename}"
                    
        except Exception as e:
            logger.error(f"Error downloading image {image_url}: {e}")
            return None
    
    def _get_file_extension(self, url: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–∑ URL"""
        parsed = urlparse(url)
        path = parsed.path.lower()
        
        if '.jpg' in path or '.jpeg' in path:
            return '.jpg'
        elif '.png' in path:
            return '.png'
        elif '.webp' in path:
            return '.webp'
        else:
            return '.jpg'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é

    async def _download_product_images(self, product_data: Dict[str, Any]) -> Dict[str, Any]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        if not product_data.get('sku'):
            return product_data
            
        sku = product_data['sku']
        downloaded_urls = []
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if product_data.get('image_url'):
            local_url = await self._download_image(product_data['image_url'], sku)
            if local_url:
                downloaded_urls.append(local_url)
                product_data['image_url'] = local_url
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if product_data.get('image_urls'):
            for img_url in product_data['image_urls'][:5]:  # –ú–∞–∫—Å–∏–º—É–º 5 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                local_url = await self._download_image(img_url, f"{sku}_{len(downloaded_urls)}")
                if local_url:
                    downloaded_urls.append(local_url)
            
            product_data['image_urls'] = downloaded_urls
        
        return product_data


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
async def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞"""
    parser = EnhancedLamodaParser(domain="kz")
    
    try:
        result = await parser.parse_catalog("nike", limit=10)
        
        print(f"\nüìä Parsing Results:")
        print(f"   Products found: {len(result)}")
        print(f"   Quality score: {parser._calculate_quality_score(result):.2f}")
        print(f"   Parsing time: {time.time() - result[0].parsed_at:.2f}s")
        
        for i, product in enumerate(result[:3], 1):
            print(f"\n{i}. {product.name}")
            print(f"   Brand: {product.brand}")
            print(f"   Price: {product.price}‚Ç∏")
            print(f"   Quality: {product.parse_quality:.2f}")
            print(f"   SKU: {product.sku}")
            
    finally:
        await parser.close()


if __name__ == "__main__":
    asyncio.run(main())
